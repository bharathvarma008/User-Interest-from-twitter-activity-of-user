{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct User Interest from twitter activity of user\n",
    "\n",
    "Problem Definition:\n",
    "As we know Twitter is one of good sources of a user’s profile, people followed, Retweets and favorites etc. The data set contains tweets from around 2000+ users which can be used tag each user with an interest.\n",
    "\n",
    "Assign user to interest buckets namely - \"Sports\", \"Business\", \"Music\" and \"Other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA:\n",
    "\n",
    "The tweets dataset is randomly collected from twitter. There are around 500,000+ tweets from 2000+ users and also the  no.of tweets from user is not unique.\n",
    "Data has been preprocessed to clean the unwanted text and to make sure the words are meaningfull to an extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tdata = pd.read_csv('tweetscopy.csv',delimiter=\";\",header= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>854630623629131776</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @balewadihstreet: Wednesdays mean weâ€™re h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>854630577156182016</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @NawabAsia: Happy Hours!! Enjoy live screen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>852456000531570688</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>Wishing all of you a very Happy &amp;amp; Prospero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>852455662051237888</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @balewadihstreet: #SignatureDish: Order sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>850620266694692866</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @NawabAsia: The #CricketFever is back with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                   1           2  \\\n",
       "0  1  854630623629131776  PreetReema   \n",
       "1  2  854630577156182016  PreetReema   \n",
       "2  3  852456000531570688  PreetReema   \n",
       "3  4  852455662051237888  PreetReema   \n",
       "4  5  850620266694692866  PreetReema   \n",
       "\n",
       "                                                   3  \n",
       "0  RT @balewadihstreet: Wednesdays mean weâ€™re h...  \n",
       "1  RT @NawabAsia: Happy Hours!! Enjoy live screen...  \n",
       "2  Wishing all of you a very Happy &amp; Prospero...  \n",
       "3  RT @balewadihstreet: #SignatureDish: Order sig...  \n",
       "4  RT @NawabAsia: The #CricketFever is back with ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total No.of Tweets: 526851\n",
    "Total No.of Users: 2057"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526851"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SlNo</th>\n",
       "      <th>ID</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>854630623629131776</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @balewadihstreet: Wednesdays mean weâ€™re h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>854630577156182016</td>\n",
       "      <td>PreetReema</td>\n",
       "      <td>RT @NawabAsia: Happy Hours!! Enjoy live screen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SlNo                  ID        User  \\\n",
       "0     1  854630623629131776  PreetReema   \n",
       "1     2  854630577156182016  PreetReema   \n",
       "\n",
       "                                                Text  \n",
       "0  RT @balewadihstreet: Wednesdays mean weâ€™re h...  \n",
       "1  RT @NawabAsia: Happy Hours!! Enjoy live screen...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.columns = [\"SlNo\", \"ID\", \"User\", \"Text\"]\n",
    "tdata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2057"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdata['User'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install/Download the required libraries/corpora from nltk using below commands. (If not exists)\n",
    "1. !pip install nltk\n",
    "2. import nltk\n",
    "3. nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Steps:\n",
    "1. Keep only Printable Text\n",
    "2. Remove White Spaces, Numbers, Hashtags(#), Stopwords, Punctuations, Tickers\n",
    "3. Convert text to Lower case\n",
    "4. Lemmatize words to its root word. etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_text = tdata['Text']\n",
    "tweet_text = tweet_text.tolist()\n",
    "\n",
    "## Text preprocessing\n",
    "#--------------------\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import itertools\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "for i in range(len(tweet_text)):\n",
    "    tweet_text[i] = filter(lambda x: x in string.printable, tweet_text[i]) #Keep only printable text\n",
    "    tweet_text[i] = re.sub(r'\\s+', ' ', tweet_text[i]) #Removing white spaces in the sentence\n",
    "    tweet_text[i] = re.sub('[\\d]', '',tweet_text[i]) #Removing numbers\n",
    "    tweet_text[i] = re.sub(r'#([^\\s]+)', r'\\1', tweet_text[i])  #Replace #word with word\n",
    "    #tweet_text[i] = \" \".join(re.findall('[A-Z][^A-Z]*', tweet_text[i]))   #Split Join words\n",
    "    tweet_text[i] = tweet_text[i].lower() #Convert to lower case\n",
    "    tweet_text[i] = \" \".join([j for j in tweet_text[i].split() if j not in stop]) #Remove Stopwords\n",
    "    tweet_text[i] = ''.join(ch for ch in tweet_text[i] if ch not in exclude) #Remove Punctuations\n",
    "    tweet_text[i] = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL',tweet_text[i]) #Convert www.* or https?://* to URL\n",
    "    tweet_text[i] = re.sub('@[^\\s]+','AT_USER',tweet_text[i])  #Convert @username to AT_USER\n",
    "    tweet_text[i] = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet_text[i]))  #Standardizing words\n",
    "    tweet_text[i] = ' '.join( [f for f in tweet_text[i].split() if len(f)>2] )\n",
    "    tweet_text[i] = tweet_text[i].strip('\\'\"')  #trim\n",
    "    tweet_text[i] = re.sub(r'\\$\\w*','',tweet_text[i]) # Remove tickers\n",
    "    #tweet_text[i] = re.sub(r'https?:\\/\\/.*\\/\\w*','',tweet_text[i]) # Remove hyperlinks\n",
    "    tweet_text[i] = \" \".join(lemmatizer.lemmatize(word,pos='v') for word in tweet_text[i].split()) # Lemmatizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_tweet=pd.DataFrame(tweet_text,columns=['Cleaned_Tweets'])\n",
    "new_data = pd.concat([tdata['User'], clean_tweet],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PreetReema</td>\n",
       "      <td>balewadihstreet wednesdays mean halfway deserv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PreetReema</td>\n",
       "      <td>nawabasia happy hours enjoy live screen match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PreetReema</td>\n",
       "      <td>wish happy amp prosperous baisakhi nawabasia h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PreetReema</td>\n",
       "      <td>balewadihstreet signaturedish order signature ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User                                     Cleaned_Tweets\n",
       "0  PreetReema  balewadihstreet wednesdays mean halfway deserv...\n",
       "1  PreetReema  nawabasia happy hours enjoy live screen match ...\n",
       "2  PreetReema  wish happy amp prosperous baisakhi nawabasia h...\n",
       "3  PreetReema  balewadihstreet signaturedish order signature ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the tweets of a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(new_data.groupby('User')['Cleaned_Tweets'].apply(lambda x: \"%s\" % ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.index.name = 'User_ID'\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001_mr</td>\n",
       "      <td>httptcokylcyblq httptcojbocgi httptcoqtdoob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>014max</td>\n",
       "      <td>andrewastor try something good today dogoodfr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User_ID                                     Cleaned_Tweets\n",
       "0  001_mr        httptcokylcyblq httptcojbocgi httptcoqtdoob\n",
       "1  014max   andrewastor try something good today dogoodfr..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2057"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if the total user(all tweets) entries in the transformed data matches with the actual no.of users\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_feed = data['Cleaned_Tweets']\n",
    "tweet_feed = tweet_feed.tolist()\n",
    "tweet_clean = [i.split() for i in tweet_feed] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Topic modeling is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using Latent Dirichlet Allocation or LDA, a popular approach to topic modeling.\n",
    "\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a vector of token counts. There are two layers in this model — documents and tokens — and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "\n",
    "1. Document vectors tend to be large (one dimension for each token $\\Rightarrow$ lots of dimensions)\n",
    "2. They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "3. The dimensions are fully indepedent from each other — there's no sense of connection between related tokens, such as knife and fork.\n",
    "\n",
    "But, as twitter has a character limit of 140 per tweet, we can use LDA to get the topic of interest as the document vector(cumulative tweets) would be small on average.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow Dirichlet probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens.\n",
    "\n",
    "<img src=\"probabilistic-topic-models-3-638.jpg\">\n",
    "\n",
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing pyLDAvis for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's Dictionary class for this.\n",
    "\n",
    "Like many NLP techniques, LDA uses a simplifying assumption known as the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(tweet_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tweet_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "intermediate_directory = os.path.join('..')\n",
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # Running and Training LDA model on the document term matrix.\n",
    "        ldamodel = Lda(doc_term_matrix, num_topics=50, id2word = dictionary, passes=50)\n",
    "    \n",
    "    ldamodel.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "ldamodel = Lda.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print u'{:20} {}'.format(u'term', u'frequency') + u'\\n'\n",
    "\n",
    "    for term, frequency in ldamodel.show_topic(topic_number, topn=25):\n",
    "        print u'{:20} {:.3f}'.format(term, round(frequency, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "mastercard           0.006\n",
      "mybigcoin            0.006\n",
      "anammirza            0.006\n",
      "dogoodfriday         0.004\n",
      "coin                 0.003\n",
      "cnbc                 0.003\n",
      "deal                 0.003\n",
      "big                  0.003\n",
      "today                0.003\n",
      "amp                  0.003\n",
      "currency             0.003\n",
      "finalize             0.003\n",
      "crypto               0.003\n",
      "httptcooqynhorwq     0.003\n",
      "cryptocurr           0.003\n",
      "pay                  0.003\n",
      "realdonaldtrump      0.003\n",
      "iampayalghosh        0.003\n",
      "marcorubio           0.003\n",
      "dosomething          0.002\n",
      "beauthentic          0.002\n",
      "buy                  0.002\n",
      "almaty               0.002\n",
      "payitforward         0.002\n",
      "httptcoimjcylkqmb    0.002\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0:'Religion',\n",
    "               1:'Music',\n",
    "               2:'Music',\n",
    "               3:'Business',\n",
    "               4:'Social Media',\n",
    "               5:'Other',\n",
    "               6:'Sports',\n",
    "               7:'Other',\n",
    "               8:'Other',\n",
    "               9:'Politics',\n",
    "              10:'Zombies',\n",
    "              11:'Pawan Kalyan',\n",
    "              12:'Feelings',\n",
    "              13:'Technology',\n",
    "              14:'eCommerce',\n",
    "              15:'Cricket',\n",
    "              16:'People',\n",
    "              17:'Moblie Phones',\n",
    "              18:'Online Games',\n",
    "              19:'Facebook',\n",
    "              20:'Competition',\n",
    "              21:'Trump',\n",
    "              22:'Other',\n",
    "              23:'fashion',\n",
    "              24:'Operating System',\n",
    "              25:'Other',\n",
    "              26:'Career',\n",
    "              27:'Startup',\n",
    "              28:'Brands',\n",
    "              29:'Money',\n",
    "              30:'Travel',\n",
    "              31:'Other',\n",
    "              32:'Cryptocurrency',\n",
    "              33:'Publicity',\n",
    "              34:'Youtube',\n",
    "              35:'Children',\n",
    "              36:'Mobile',\n",
    "              37:'Other',\n",
    "              38:'Politics',\n",
    "              39:'Social Media',\n",
    "              40:'Emotions',\n",
    "              41:'Europe',\n",
    "              42:'Poetry',\n",
    "              43:'Fashion',\n",
    "              44:'Music Event',\n",
    "              45:'Casino',\n",
    "              46:'sports',\n",
    "              47:'Adventures',\n",
    "              48:'Fashion',\n",
    "              49:'News'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'w') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis includes a one-line function to take topic models created with gensim and prepare their data for visualization.\n",
    "Check: https://github.com/bmabey/pyLDAvis \n",
    "\n",
    "As the model takes a lot of time to process, visualizations take time to be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda2\\lib\\site-packages\\pyLDAvis\\_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    }
   ],
   "source": [
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix,dictionary)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lda_description(tweet, min_topic_freq=0.05):\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    tweet_bow = dictionary.doc2bow(tweet)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    tweet_lda = ldamodel[tweet_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    tweet_lda = sorted(tweet_lda, key=lambda (topic_number, freq): -freq)\n",
    "    \n",
    "    #return tweet_lda[0]\n",
    "    topic_list=[]\n",
    "    for topic_number, freq in tweet_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        topic_list.append(topic_names[topic_number]) \n",
    "        #print '{:25} {}'.format(topic_names[topic_number], round(freq, 2))\n",
    "        \n",
    "    if len(topic_list) >= 1:\n",
    "        return topic_list[0]\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Career'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_description(tweet_clean[75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding topic of interest for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_interest = []\n",
    "user_interest = [lda_description(tweets) for tweets in tweet_clean]\n",
    "user_interest =pd.DataFrame(user_interest,columns=['Interest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users = new_data['User'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['001_mr', '014max', '0192474822j', ..., 'zoezackbella',\n",
       "       'zombieman5000', 'zsiloamcrutcher'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "users = np.sort(users, axis=None)\n",
    "users\n",
    "users=pd.DataFrame(users,columns=['User'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the user-to-interest mapping !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_df = pd.concat([users, user_interest],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Interest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001_mr</td>\n",
       "      <td>Career</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>014max</td>\n",
       "      <td>Cryptocurrency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0192474822j</td>\n",
       "      <td>Feelings</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05mer05</td>\n",
       "      <td>Zombies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09Dimon13</td>\n",
       "      <td>People</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          User        Interest\n",
       "0       001_mr          Career\n",
       "1       014max  Cryptocurrency\n",
       "2  0192474822j        Feelings\n",
       "3      05mer05         Zombies\n",
       "4    09Dimon13          People"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"users_interest.csv\", user_df, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Competition'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_interest[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AZTIECA'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the task was to assign user to interest buckets namely - \"Sports\", \"Business\", \"Music\" and \"Other\". \n",
    "Check for all the topics and assign accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Categorizing based on specific topics (Sports, Music, Business and Others) only:\n",
    "\n",
    "topics_others = ['Religion','Other', 'Politics', 'Zombies', 'Pawan Kalyan', 'Feelings', 'People', \n",
    "                 'Trump', 'fashion', 'Career', 'Travel', 'Children', \n",
    "                 'Emotions', 'Europe', 'Poetry', 'Fashion', 'News']\n",
    "\n",
    "topics_Sports = ['Sports', 'Cricket', 'Online Games', 'Competition', 'Casino', 'Adventures']\n",
    "\n",
    "topics_Music = ['Music', 'Music Event']\n",
    "\n",
    "topics_Business = ['Business',  'Technology', 'eCommerce', 'Moblie Phones', 'Facebook', 'Operating System', \n",
    "                    'Startup', 'Brands', 'Money', 'Cryptocurrency', 'Publicity', 'Youtube',\n",
    "                    'Mobile', 'Social Media']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_categories = []\n",
    "for i in range(len(user_interest)):\n",
    "    if any(user_interest['Interest'][i] in word for word in topics_others):\n",
    "        user_categories.append('Other')\n",
    "    elif any(user_interest['Interest'][i] in word for word in topics_Sports):\n",
    "        user_categories.append('Sports')\n",
    "    elif any(user_interest['Interest'][i] in word for word in topics_Music):\n",
    "        user_categories.append('Music')\n",
    "    elif any(user_interest['Interest'][i] in word for word in topics_Business):\n",
    "        user_categories.append('Business')\n",
    "    else:\n",
    "        user_categories.append('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_categories =pd.DataFrame(user_categories,columns=['Interest'])\n",
    "user_df1 = pd.concat([users, user_categories],axis=1)\n",
    "user_df1.columns = [\"User\", \"Interest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save final document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"users_categorized.csv\", user_df1, delimiter=\",\", fmt='%s', header=\"User, Interest\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
